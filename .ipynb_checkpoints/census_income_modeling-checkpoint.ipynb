{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census Income Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "import pipeline_utilities as p_util\n",
    "#import pickle\n",
    "#from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('probabilities.pkl', 'rb') as file:\n",
    "#    model = pickle.load(file)\n",
    "#    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and examine the training dataset\n",
    "train_data = pd.read_csv(\"../Project2_Resources/census-income-train.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and examine the test dataset\n",
    "test_data = pd.read_csv(\"../Project2_Resources/census-income-test.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the columns\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the values\n",
    "#train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function\n",
    "def set_target(above50k):\n",
    "    if above50k == '-50000':\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "# \"Apply\" the function to the amount column in the two data sets\n",
    "train_data['KTarget'] = train_data['Above50K'].apply(set_target)\n",
    "train_data['KTarget'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['KTarget'] = test_data['Above50K'].apply(set_target)\n",
    "test_data['KTarget'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = ['Class', 'Education', 'Education last week', 'Marital',\n",
    "                     'Major Industry', 'Major Occupation', 'Race', 'Employment Status',\n",
    "                     'Hispanic', 'Gender', 'Labor union', 'Unemployment Reason', \n",
    "                     'TaxFiler', 'Previous Region', 'Previous State', 'Family Status', \n",
    "                     'Household', 'MIGMSA', 'MIGRegion', 'MIGMove',\n",
    "                     '1YearAgo', 'PrevSunBelt', 'Parents', 'FatherCountry', 'MotherCountry', \n",
    "                     'BirthCountry', 'Citizenship', 'Self employed', \n",
    "                     'VetQtnaire', 'Veteran' \n",
    "                    ]\n",
    "\n",
    "# Make a copy of the datasets\n",
    "train_data_copy = train_data.copy()\n",
    "test_data_copy = test_data.copy()\n",
    "\n",
    "# Loop through columns_to_encode and convert the columns to category codes\n",
    "for column in columns_to_encode:\n",
    "    train_data_copy[column] = train_data_copy[column].astype(\"category\").cat.codes\n",
    "    test_data_copy[column] = test_data_copy[column].astype(\"category\").cat.codes\n",
    "\n",
    "train_data_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data_copy.drop(columns=['Above50K', 'KTarget'])\n",
    "X_test = test_data_copy.drop(columns=['Above50K', 'KTarget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_copy[['KTarget', 'Capital Gains', 'Parents', 'Veteran', 'Family Status', 'Industry', 'Weeks Worked', 'Wage per hour']].corr()\n",
    "print(train_data_copy.drop(columns='Above50K').corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the labels set `y` and features DataFrame `X`\n",
    "y_train = train_data_copy['KTarget']\n",
    "y_test = test_data_copy['KTarget']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the balance of the labels variable (`y`) by using the `value_counts` function.\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X_train, X_test, y_train, y_test\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `StandardScaler` to scale the features data. Remember that only `X_train` and `X_test` DataFrames should be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the test dataset based on the fit from the training dataset\n",
    "X_train_scaled, X_test_scaled = p_util.scale_data_with_StandardScaler(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "p_util.gradient_boost_model_generator(X_train_scaled, X_test_scaled, y_train, y_test, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "p_util.ada_boost_model_generator(X_train_scaled, X_test_scaled, y_train, y_test, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "random_state = 1\n",
    "p_util.extra_trees_model_generator(X_train_scaled, X_test_scaled, y_train, y_test, random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_util.decision_tree_model_generator(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit a PCA Model\n",
    "\n",
    "Try a PCA model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_data_pca = pca.fit_transform(X_train_scaled)\n",
    "X_data_pca[:5]\n",
    "pca_df = pd.DataFrame(\n",
    "    X_data_pca,\n",
    "    columns=[\"PCA1\", \"PCA2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_pca[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "inertia = []\n",
    "k = list(range(1, 11))\n",
    "for i in k:\n",
    "    k_model = KMeans(n_clusters=i, n_init='auto', random_state=1)\n",
    "    k_model.fit(pca_df)\n",
    "    inertia.append(k_model.inertia_)\n",
    "\n",
    "# Define a DataFrame to hold the values for k and the corresponding inertia\n",
    "elbow_data = {\"k\": k, \"inertia\": inertia}\n",
    "df_elbow = pd.DataFrame(elbow_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the DataFrame\n",
    "df_elbow.head()\n",
    "df_elbow.plot.line(\n",
    "    x=\"k\", \n",
    "    y=\"inertia\", \n",
    "    title=\"Elbow Curve\", \n",
    "    xticks=k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the rate of decrease between each k value\n",
    "k = df_elbow[\"k\"]\n",
    "inertia = df_elbow[\"inertia\"]\n",
    "for i in range(1, len(k)):\n",
    "    percentage_decrease = (inertia[i-1] - inertia[i]) / inertia[i-1] * 100\n",
    "    print(f\"Percentage decrease from k={k[i-1]} to k={k[i]}: {percentage_decrease:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with 5 clusters\n",
    "model = KMeans(n_clusters=4, n_init='auto', random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(pca_df)\n",
    "\n",
    "# Make predictions\n",
    "k_3 = model.predict(pca_df)\n",
    "\n",
    "# Create a copy of the PCA DataFrame\n",
    "pca_predictions_df = pca_df.copy()\n",
    "\n",
    "# Add a class column with the labels\n",
    "pca_predictions_df[\"income_segments\"] = k_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_predictions_df.plot.scatter(\n",
    "    x=\"PCA1\",\n",
    "    y=\"PCA2\",\n",
    "    c=\"income_segments\",\n",
    "    colormap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_component_weights = pd.DataFrame(pca.components_.T, columns=['PCA1', 'PCA2'], index=X_train.columns)\n",
    "pca_component_weights.sort_values('PCA1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit a Logistic Regression Model\n",
    "\n",
    "Create a Logistic Regression model, fit it to the training data, make predictions with the testing data, and print the model's accuracy score. You may choose any starting settings you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All requirements above have been coded into pipeline_utilities python program file\n",
    "\n",
    "random_state = 1\n",
    "p_util.logistic_regression_model_generator(X_train_scaled, X_test_scaled, y_train, y_test, random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit a Random Forest Classifier Model\n",
    "\n",
    "Create a Random Forest Classifier model, fit it to the training data, make predictions with the testing data, and print the model's accuracy score. You may choose any starting settings you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All details have been coded into pipeline_utilities python program file\n",
    "# Tried 500 estimators, almost no difference and balanced accuracy score was slightly worse, putting it back to 100\n",
    "\n",
    "random_state = 1\n",
    "n_estimators = 100\n",
    "p_util.random_forest_model_generator(X_train_scaled, X_test_scaled, y_train, y_test, random_state, n_estimators, X_train.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit an SVM Model\n",
    "\n",
    "Create a Support Vector Machine model, fit it to the training data, make predictions with the testing data, and print the model's accuracy score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All details have been coded into pipeline_utilities python program file, takes FOREVER to run\n",
    "\n",
    "#kernel_type = 'linear'\n",
    "#p_util.svm_model_generator(X_train_scaled, X_test_scaled, y_train, y_test, kernel_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings and Conclusions\n",
    "\n",
    "What was the result of your analysis? Which model performed better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All models that I tried achieved over 90% accuracy score with testing data and predictions. The Random Forest model had the best F1 scores of 52%-98% with Gradient Boost running a close second at 51%-98%. PCA analysis resulted in a very pretty set of 4 segments, but the total explained variance by PCA1 and PCA2 was 0.27, so did not reduce dimensionality effectively. My SVC model took forever to run. I ran a total of 6 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
